{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "damaged-assurance",
   "metadata": {},
   "source": [
    "# Day 3 Notebook\n",
    "This notebook contains two exercises: building and training a CNN with image data from CIFAR10 and building and training an RNN for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-albuquerque",
   "metadata": {},
   "source": [
    "## Part 1: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-register",
   "metadata": {},
   "source": [
    "### Classifying Images\n",
    "In this exercise we'll build a CNN to classify images from the [CIFAR benchmark dataset](https://www.cs.toronto.edu/~kriz/cifar.html). This lesson is adapted from the [Keras Tutorial](https://www.tensorflow.org/tutorials/images/cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "!pip install --ignore-installed --upgrade tensorflow==2.2.0\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-sheep",
   "metadata": {},
   "source": [
    "Now we'll download the CIFAR10 dataset. This dataset contains 60,000 color images in 10 classes, divided into 50,000 training images and 10,000 validation images. The 10 classes are all mutually exclusive so there is no overlap in the labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-despite",
   "metadata": {},
   "source": [
    "Let's take a look at the first 25 images in the training dataset and their corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map of the class names (instead of numerical labels)\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-swing",
   "metadata": {},
   "source": [
    "Now we can start building our model! We'll define three sets of Convolutional and MaxPooling layers.  \n",
    "\n",
    "The CNN will take input tensors (3D matrices) of shape (image_height, image_width, color_channels). For the CIFAR10 dataset the images are 32x32 pixels with the standard 3 color channels (RGB) so the input dimensions will be (32, 32, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-bouquet",
   "metadata": {},
   "source": [
    "We can use the `model.summary()` function to see if our CNN looks the way we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-democracy",
   "metadata": {},
   "source": [
    "From this model summary we see that indeed we have three convolutional layers, the first learns 32 filters of size 3x3 and the next two each leran 64 filters of size 3x3. Typically, as your image representations get smaller (in deeper layers of the network) you can have more learnable filters because there is less computational power required to apply the filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-tuesday",
   "metadata": {},
   "source": [
    "To complete our model, we need to add dense layers that will take the new image representations from our convolutions and perform the classification on them. Standard dense layers take input vectors (which are 1D) so we'll have to flatten our 3D image representations into a vector for input. As a reminder, CIFAR10 has 10 classes, so our final output layer should have 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-theater",
   "metadata": {},
   "source": [
    "here's our complete model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-identifier",
   "metadata": {},
   "source": [
    "Now we can train our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-thinking",
   "metadata": {},
   "source": [
    "And evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-blind",
   "metadata": {},
   "source": [
    "Nice! We get a pretty good accuracy with a pretty basic CNN! Try some of the exercises below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-coffee",
   "metadata": {},
   "source": [
    "**Exercise:** Adjust one of more hyperparameters of your CNN (examples: filter size, number of layers, pooling function, learning rate, etc) and compare the performance to the network we built above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-airplane",
   "metadata": {},
   "source": [
    "We can also take a look at the layers of our CNN. Each layer has a `layer_name` property where the individual layers have naming conventions like 'block#conv#' where the # is a integer. We can also use `get_weights` to get the actual weight values of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-dance",
   "metadata": {},
   "source": [
    "**Exercise:** Visualize the filters in the first convolutional layer (Note, you should normalize the values to be between 0 and 1). Can you make any inferences about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-think",
   "metadata": {},
   "source": [
    "## Part Two: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-tourist",
   "metadata": {},
   "source": [
    "Now we'll train an RNN on text data. This lesson is adapted from the [Keras Tutorial](https://www.tensorflow.org/tutorials/text/text_classification_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-investigation",
   "metadata": {},
   "source": [
    "We'll use data from the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/). This is a binary classification dataset where each review has been labeled as experessing either a positive or negative sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-astrology",
   "metadata": {},
   "source": [
    "Let's take a look at a sample review: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:3])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-withdrawal",
   "metadata": {},
   "source": [
    "Now we need to create a text encoder. We can use the Keras `experimental.preprocessing.TextVectorization` to do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-delaware",
   "metadata": {},
   "source": [
    "We can take a look at the top 20 most common words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-moderator",
   "metadata": {},
   "source": [
    "Once the vocabulary is set the layer encodes the text into numerical indices. Each review will now be a tensor and each tensor is 0-padded to the length of the longest review in the dataset (unless we specify a fixed `output_sequence_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder(example.numpy()[:3])\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", example[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-discrimination",
   "metadata": {},
   "source": [
    "We'll implement a model with the following architecture: \n",
    "\n",
    "1. This model can be build as a tf.keras.Sequential.\n",
    "\n",
    "2. The first layer is the encoder, which converts the text to a sequence of token indices.\n",
    "\n",
    "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
    "\n",
    "This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a tf.keras.layers.Dense layer.\n",
    "\n",
    "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
    "\n",
    "The tf.keras.layers.Bidirectional wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output.\n",
    "\n",
    "The main advantage to a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.\n",
    "\n",
    "The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
    "\n",
    "5. After the RNN has converted the sequence to a single vector the two layers.Dense do some final processing, and convert from this vector representation to a single logit as the classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None,1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-howard",
   "metadata": {},
   "source": [
    "**Exercise:** test the trained model on a new review sentence you write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-fellow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
